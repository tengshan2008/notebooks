{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-alpha0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing Ops Eagerly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[4]], shape=(1, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = [[2,]]\n",
    "m = tf.square(x)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a * b = \n",
      " [[ 8  9]\n",
      " [18 19]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1, 2],\n",
    "                 [3, 4]])\n",
    "b = tf.constant([[2, 1],\n",
    "                 [3, 4]])\n",
    "ab = tf.matmul(a, b)\n",
    "print('a * b = \\n', ab.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[2 3]], shape=(1, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[2, 3]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception raised\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    a.assign(2.)\n",
    "except:\n",
    "    print('Exception raised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old value for v = 5.0\n",
      "New value for v = 2.0\n"
     ]
    }
   ],
   "source": [
    "v = tf.Variable(5.)\n",
    "print('Old value for v =', v.numpy())\n",
    "v.assign(2.)\n",
    "print('New value for v =', v.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value     :  2.0\n",
      "increment :  3.0\n",
      "decrement :  2.0\n"
     ]
    }
   ],
   "source": [
    "v.assign(2.)\n",
    "print('value     : ', v.numpy())\n",
    "print('increment : ', v.assign_add(1.).numpy())\n",
    "print('decrement : ', v.assign_sub(1.).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name  :  Variable:0\n",
      "type  :  <dtype: 'float32'>\n",
      "shape :  ()\n",
      "device:  /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print('name  : ', v.name)\n",
    "print('type  : ', v.dtype)\n",
    "print('shape : ', v.shape)\n",
    "print('device: ', v.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient of w^2 at 2.0 is 4.0\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(2.0)\n",
    "\n",
    "#watch the gradient of the loss operation\n",
    "with tf.GradientTape() as tape:\n",
    "  loss = w * w\n",
    "\n",
    "grad = tape.gradient(loss, w)\n",
    "print('The gradient of w^2 at {} is {}'.format(w.numpy(), grad.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+tf.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient of the sigmoid function at 0.0 is  0.25\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(0.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = sigmoid(x)\n",
    "    \n",
    "print('The gradient of the sigmoid function at 0.0 is ', tape.gradient(y,x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(x):\n",
    "    return tf.math.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first derivative of log at x = 1 is  1.0\n",
      "The second derivative of log at x = 1 is  -1.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(1.)\n",
    "\n",
    "with tf.GradientTape() as t1:\n",
    "    with tf.GradientTape() as t2:\n",
    "        y = log(x)\n",
    "    dx = t2.gradient(y, x)\n",
    "dx2 = t1.gradient(dx, x)\n",
    "\n",
    "print('The first derivative of log at x = 1 is ', dx.numpy())\n",
    "print('The second derivative of log at x = 1 is ', dx2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logexp(x):\n",
    "    return tf.math.log(1 + tf.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_logexp(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = logexp(x)\n",
    "    dx = tape.gradient(y, x)\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient at x = 0 is  0.5\n",
      "The gradient at x = 100 is  nan\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(0.)\n",
    "print('The gradient at x = 0 is ', grad_logexp(x).numpy())\n",
    "x = tf.Variable(100.)\n",
    "print('The gradient at x = 100 is ', grad_logexp(x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def logexp_stable(x):\n",
    "    e = tf.exp(x)\n",
    "    def grad(dy):\n",
    "        return dy * (1 - 1 / (1 + e))\n",
    "    return tf.math.log(1 + e), grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_logexp(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = logexp_stable(x)\n",
    "    dx = tape.gradient(y, x)\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient at x = 100 is  1.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(100.)\n",
    "print('The gradient at x = 100 is ', grad_logexp(x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def g(x):\n",
    "    y = tf.reduce_sum(x)\n",
    "    if y > 0:\n",
    "        return y\n",
    "    return tf.abs(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=183, shape=(), dtype=int32, numpy=3>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean loss is  3.0\n"
     ]
    }
   ],
   "source": [
    "m = tf.keras.metrics.Mean(\"loss\")\n",
    "\n",
    "m(2)\n",
    "m(4)\n",
    "\n",
    "print('The mean loss is ', m.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean loss is  0.0\n"
     ]
    }
   ],
   "source": [
    "m.reset_states()\n",
    "print('The mean loss is ', m.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 000: 13.920\n",
      "Loss at step 020: 6.751\n",
      "Loss at step 040: 3.551\n",
      "Loss at step 060: 2.123\n",
      "Loss at step 080: 1.485\n",
      "Loss at step 100: 1.201\n",
      "Loss at step 120: 1.074\n",
      "Loss at step 140: 1.017\n",
      "Loss at step 160: 0.992\n",
      "Loss at step 180: 0.980\n",
      "W : 2.923673391342163, b = 1.991908311843872\n"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES = 1000\n",
    "\n",
    "X = tf.random.normal([NUM_EXAMPLES])\n",
    "noise = tf.random.normal([NUM_EXAMPLES])\n",
    "y = X * 3 + 2 + noise\n",
    "\n",
    "W = tf.Variable(0.)\n",
    "b = tf.Variable(0.)\n",
    "\n",
    "train_steps = 200\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(train_steps):\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        yhat = X * W + b\n",
    "        \n",
    "        error = yhat - y\n",
    "        loss = tf.reduce_mean(tf.square(error))\n",
    "        \n",
    "    dW, db = tape.gradient(loss, [W, b])\n",
    "    \n",
    "    W.assign_sub(dW * learning_rate)\n",
    "    b.assign_sub(db * learning_rate)\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss at step {:03d}: {:.3f}\".format(i, loss))\n",
    "        \n",
    "print(\"W : {}, b = {}\".format(W.numpy(), b.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Processing and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = x_train[2:3]\n",
    "img = np.expand_dims(img, -1)\n",
    "\n",
    "# flipping horizontally\n",
    "img1 = tf.image.flip_left_right(img)\n",
    "\n",
    "# flipping vertically\n",
    "img2 = tf.image.flip_up_down(img)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(tf.concat([img, img1, img2], axis = 2).numpy().squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(x, y):\n",
    "    # normalize and expand\n",
    "    x = tf.cast(x, tf.float32)/255.\n",
    "    x = tf.expand_dims(x, -1)\n",
    "    \n",
    "    # cast the labels\n",
    "    y = tf.cast(y, tf.int32)\n",
    "    return x, y\n",
    "\n",
    "def create_dataset(x, y):\n",
    "    # convert to tensors and shuffle\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y)).shuffle(len(x)-1)\n",
    "    \n",
    "    # extract batches\n",
    "    dataset = dataset.batch(32)\n",
    "    \n",
    "    # preprocess the batch\n",
    "    dataset = dataset.map(pre_process, num_parallel_calls=4)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(x_train, y_train)\n",
    "test_dataset = create_dataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for x, y in train_dataset:\n",
    "    plt.imshow(x[0].numpy().squeeze())\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.random_uniform_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03533089  0.01183294 -0.03340511  0.03154682 -0.02579676 -0.03091618\n",
      "  0.03413266 -0.04336311 -0.04530348 -0.038062  ]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(initializer(shape = (10,)))\n",
    "print(x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        \n",
    "        # random initializer for the weights\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, units), dtype='float32'),\n",
    "            trainable=True)\n",
    "        \n",
    "        # zero initialized biases\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value=b_init(shape=(units,), dtype='float32'),\n",
    "            trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # froward call\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a simple linear layer that has 10 neorons and accept inputs of size 3\n",
    "llayer = Linear(units = 10, input_dim = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8597, shape=(3, 10), dtype=float32, numpy=\n",
       "array([[ 0.01016862, -0.22906333,  0.01520133, -0.02347573,  0.07029915,\n",
       "        -0.10171644,  0.05988783,  0.15682419, -0.1622899 ,  0.10910574],\n",
       "       [ 0.01016862, -0.22906333,  0.01520133, -0.02347573,  0.07029915,\n",
       "        -0.10171644,  0.05988783,  0.15682419, -0.1622899 ,  0.10910574],\n",
       "       [ 0.01016862, -0.22906333,  0.01520133, -0.02347573,  0.07029915,\n",
       "        -0.10171644,  0.05988783,  0.15682419, -0.1622899 ,  0.10910574]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward call of batch of size 3\n",
    "llayer(tf.ones((3, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End CNN Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Convolution2D, MaxPooling2D, Flatten, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters = 16, kernel_size = 3, padding = 'same', input_shape = [28, 28, 1], activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=100, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=10,activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               156900    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 162,710\n",
      "Trainable params: 162,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8781, shape=(10, 10), dtype=float32, numpy=\n",
       "array([[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "model(np.zeros((10, 28, 28, 1), np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design the loss function, Gradient and Accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the loss\n",
    "def loss(y, yhat):\n",
    "    # this applies the loss to sparse labels i.e not one hot encoded\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(y, yhat)\n",
    "\n",
    "# record the gradient with respect to the model variables\n",
    "def grad(model, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        yhat = model(x)\n",
    "        loss_value = loss(y, yhat)\n",
    "    return tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "# calcuate the accuracy of the model\n",
    "def accuracy(y, yhat):\n",
    "    # get the labels of the predicted values\n",
    "    yhat = tf.argmax(yhat, 1).numpy()\n",
    "    \n",
    "    # get the labels of the true values\n",
    "    y = y.numpy()\n",
    "    return np.sum(y == yhat)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intitalize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# record epoch loss and accuracy\n",
    "loss_history = tf.keras.metrics.Mean('loss')\n",
    "accuracy_history = tf.keras.metrics.Mean('accuracy')\n",
    "\n",
    "# epochs\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 Loss: 0.103, Acc: 0.968\n",
      "epoch: 2 Loss: 0.033, Acc: 0.989\n",
      "epoch: 3 Loss: 0.023, Acc: 0.993\n"
     ]
    }
   ],
   "source": [
    "for epoch in  range(1, epochs+1):\n",
    "    for x, y in train_dataset:\n",
    "        \n",
    "        yhat = model(x)\n",
    "        # Calculate derivatives of the input function with respect to its parameters\n",
    "        grads = grad(model, x, y)\n",
    "        \n",
    "        # Apply the gradient to the model\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        # record the current loss and accuracy\n",
    "        loss_history(loss(y, yhat))\n",
    "        accuracy_history(accuracy(y, yhat))\n",
    "        \n",
    "    print(\"epoch: {:d} Loss: {:.3f}, Acc: {:.3f}\".format(epoch, loss_history.result(), accuracy_history.result()))\n",
    "    \n",
    "    # clear the history\n",
    "    loss_history.reset_states()\n",
    "    accuracy_history.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.99\n"
     ]
    }
   ],
   "source": [
    "accuracy_history.reset_states()\n",
    "for x, y in test_dataset:\n",
    "    # Calculate derivatives of the input function with respect to its parameters\n",
    "    yhat = model(x)\n",
    "    \n",
    "    # record the current loss and accuracy\n",
    "    accuracy_history(accuracy(y, yhat))\n",
    "print(\"Acc: {:.2f}\".format(accuracy_history.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Restore a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# check if it exists\n",
    "if os.path.isdir('model'):\n",
    "    raise Exception('Folder exists !')\n",
    "else:\n",
    "    checkpoint_dir = 'model'\n",
    "    os.mkdir(checkpoint_dir)\n",
    "    \n",
    "    # create a root for the checkpoint\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "    root = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "    \n",
    "    # save the model\n",
    "    root.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty model\n",
    "model = create_model()\n",
    "\n",
    "# create a checkpoint variable\n",
    "root = tf.train.Checkpoint(optimizer=optimizer, model=create_model())\n",
    "\n",
    "# restores the model\n",
    "root.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "# retrieve the trained model\n",
    "model = root.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
